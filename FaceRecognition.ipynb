{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48eec5a5-1c2c-4998-9ab2-2e13bd185b5b",
   "metadata": {},
   "source": [
    "# 1.Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c30f6b-add7-4afe-9dce-6f3a92de13fc",
   "metadata": {},
   "source": [
    "### 1.1 Installing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f692941-9c78-4adb-9e1a-ee5608d2e317",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow opencv-python matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc26461-b9f8-41bd-9841-176e280abc63",
   "metadata": {},
   "source": [
    "### 1.2 collect Images Using opencv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f7cfdfe-6b06-4919-b4c7-8827cd71335a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer, Conv2D, Dense, MaxPooling2D, Input, Flatten\n",
    "import tensorflow as tf\n",
    "import uuid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0b69a9-5d4e-4434-a216-6031ed7d0889",
   "metadata": {},
   "source": [
    "### 1.4 Create Folder Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea43d45a-81a5-4626-b842-03af8d9d6e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup paths\n",
    "POS_PATH = os.path.join('data', 'positive')\n",
    "NEG_PATH = os.path.join('data', 'negative')\n",
    "ANC_PATH = os.path.join('data', 'anchor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d824746-68a7-444b-b2d7-7b0c707f28ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the directories\n",
    "os.makedirs(POS_PATH, exist_ok=True)\n",
    "os.makedirs(NEG_PATH, exist_ok=True)\n",
    "os.makedirs(ANC_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f7f669-ee89-4406-b24a-3ee2b6238cac",
   "metadata": {},
   "source": [
    "# 2. Collect Positives and Anchors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc04488-601a-4d7e-b50f-7a2fc259ce4f",
   "metadata": {},
   "source": [
    "### 2.1 Untar Labelled Faces in the Wild Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31864d0c-4304-4798-90f9-96a0268ba293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://vis-www.cs.umass.edu/lfw/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76c52d45-8c18-466d-a4f4-102517aae7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncompress Tar GZ Labelled Faces in the Wild Dataset\n",
    "!tar -xf lfw.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a309135-39b9-495b-9f5d-13fe12902517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move LFW Images to the following repository data/negative\n",
    "for directory in os.listdir('lfw'):\n",
    "    for file in os.listdir(os.path.join('lfw', directory)):\n",
    "        EX_PATH = os.path.join('lfw', directory, file)\n",
    "        NEW_PATH = os.path.join(NEG_PATH, file)\n",
    "        os.replace(EX_PATH, NEW_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea4e407-1f81-47d0-afb0-daaec63c48e6",
   "metadata": {},
   "source": [
    "### 2.2 Collect Positive and Anchor Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4a845fb-ea21-467e-916a-15bcde3e74b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import uuid library to generate unique image names\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46fb96ec-e5e3-4cf5-95ef-60cf4584576e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data\\\\anchor\\\\fff8308c-376d-11ef-a5f8-5414f35162e7.jpg'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(ANC_PATH, '{}.jpg'.format(uuid.uuid1()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72baad60-83f2-4593-9c0c-fd6de2c3c5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "# Function to capture images with names\n",
    "def capture_images(label):\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    while cap.isOpened(): \n",
    "        ret, frame = cap.read()\n",
    "        frame = frame[120:120+250, 200:200+250, :]\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('a'):\n",
    "            imgname = os.path.join(ANC_PATH, '{}_{}.jpg'.format(label, uuid.uuid1()))\n",
    "            cv2.imwrite(imgname, frame)\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('p'):\n",
    "            imgname = os.path.join(POS_PATH, '{}_{}.jpg'.format(label, uuid.uuid1()))\n",
    "            cv2.imwrite(imgname, frame)\n",
    "        \n",
    "        cv2.imshow('Image Collection', frame)\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a6664039-a304-47d2-a323-2fb70f9cdaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage: capture images for a person named \"John\"\n",
    "capture_images(\"Mahesh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98189984-a6d3-4e74-9a3e-e527ac448c97",
   "metadata": {},
   "source": [
    "### 2.3 NEW - Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "99b6088f-1347-4089-9100-b6ee8eef853f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_aug(img):\n",
    "    data = []\n",
    "    for i in range(9):\n",
    "        img = tf.image.stateless_random_brightness(img, max_delta=0.02, seed=(1,2))\n",
    "        img = tf.image.stateless_random_contrast(img, lower=0.6, upper=1, seed=(1,3))\n",
    "        # img = tf.image.stateless_random_crop(img, size=(20,20,3), seed=(1,2))\n",
    "        img = tf.image.stateless_random_flip_left_right(img, seed=(np.random.randint(100),np.random.randint(100)))\n",
    "        img = tf.image.stateless_random_jpeg_quality(img, min_jpeg_quality=90, max_jpeg_quality=100, seed=(np.random.randint(100),np.random.randint(100)))\n",
    "        img = tf.image.stateless_random_saturation(img, lower=0.9,upper=1, seed=(np.random.randint(100),np.random.randint(100)))\n",
    "            \n",
    "        data.append(img)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e94fcbf7-6e12-458c-804c-f80601b8374c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process images in ANC_PATH\n",
    "for file_name in os.listdir(ANC_PATH):\n",
    "    img_path = os.path.join(ANC_PATH, file_name)\n",
    "    img = cv2.imread(img_path)\n",
    "    \n",
    "    if img is not None:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert image to RGB format\n",
    "        img = tf.convert_to_tensor(img, dtype=tf.float32) / 255.0  # Normalize image\n",
    "        augmented_images = data_aug(img)\n",
    "\n",
    "        for i, image in enumerate(augmented_images):\n",
    "            # Generate a new filename with original label and UUID\n",
    "            new_file_name = '{}_{}.jpg'.format(file_name.split('.')[0], uuid.uuid1())\n",
    "            save_path = os.path.join(ANC_PATH, new_file_name)\n",
    "\n",
    "            # Save the augmented image\n",
    "            cv2.imwrite(save_path, (image.numpy() * 255).astype(np.uint8))\n",
    "    else:\n",
    "        print(f\"Failed to load image at path: {img_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d44b03cc-85e8-431e-b130-201b69cf4262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process images in POS_PATH\n",
    "for file_name in os.listdir(POS_PATH):\n",
    "    img_path = os.path.join(POS_PATH, file_name)\n",
    "    img = cv2.imread(img_path)\n",
    "    \n",
    "    if img is not None:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert image to RGB format\n",
    "        img = tf.convert_to_tensor(img, dtype=tf.float32) / 255.0  # Normalize image\n",
    "        augmented_images = data_aug(img)\n",
    "\n",
    "        for i, image in enumerate(augmented_images):\n",
    "            # Generate a new filename with original label and UUID\n",
    "            new_file_name = '{}_{}.jpg'.format(file_name.split('.')[0], uuid.uuid1())\n",
    "            save_path = os.path.join(POS_PATH, new_file_name)\n",
    "\n",
    "            # Save the augmented image\n",
    "            cv2.imwrite(save_path, (image.numpy() * 255).astype(np.uint8))\n",
    "    else:\n",
    "        print(f\"Failed to load image at path: {img_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76af328f-ddec-4ef8-a30c-87280b95ead4",
   "metadata": {},
   "source": [
    "# 3. Load and Preprocess Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89e3766-6116-451d-ab8c-5a769e2fd75e",
   "metadata": {},
   "source": [
    "### 3.1 Get Image Directories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b344bae-6242-41c3-b3ee-a08b938cfa65",
   "metadata": {},
   "source": [
    "#### 3.2 Preprocessing - Scale and Resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b1697cc3-92c7-449e-bbc9-b43b8c772e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(file_path):\n",
    "    byte_img = tf.io.read_file(file_path)\n",
    "    img = tf.io.decode_jpeg(byte_img)\n",
    "    img = tf.image.resize(img, (100, 100))\n",
    "    img = img / 255.0\n",
    "    return img\n",
    "\n",
    "anchor = tf.data.Dataset.list_files(os.path.join(ANC_PATH, '*.jpg')).take(3000)\n",
    "positive = tf.data.Dataset.list_files(os.path.join(POS_PATH, '*.jpg')).take(3000)\n",
    "negative = tf.data.Dataset.list_files(os.path.join(NEG_PATH, '*.jpg')).take(3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09c8d4d-9a00-4bcb-b129-7392e2f40fb9",
   "metadata": {},
   "source": [
    "### 3.3 Create Labelled Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab6a962-e6c9-4093-84e9-771281a9695b",
   "metadata": {},
   "source": [
    "### 3.4 Build Train and Test Partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "99b2008a-18ad-4c98-86e8-ae4fbebaf74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_twin(input_img, validation_img, label):\n",
    "    return(preprocess(input_img), preprocess(validation_img), label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "23d17335-9975-4676-97c7-f1d882be68f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "positives = tf.data.Dataset.zip((anchor, positive, tf.data.Dataset.from_tensor_slices(tf.ones(len(anchor)))))\n",
    "negatives = tf.data.Dataset.zip((anchor, negative, tf.data.Dataset.from_tensor_slices(tf.zeros(len(anchor)))))\n",
    "data = positives.concatenate(negatives)\n",
    "\n",
    "data = data.map(preprocess_twin)\n",
    "data = data.cache()\n",
    "data = data.shuffle(buffer_size=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9fd663e0-baf0-48cb-950e-e265fbd898cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_ShuffleDataset element_spec=(TensorSpec(shape=(100, 100, None), dtype=tf.float32, name=None), TensorSpec(shape=(100, 100, None), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.float32, name=None))>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1341cc89-53e6-475e-8048-9bea4d9f6116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training partition\n",
    "train_data = data.take(round(len(data)*.7))\n",
    "train_data = train_data.batch(16)\n",
    "train_data = train_data.prefetch(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b06259a9-3d96-4400-b784-514d6b501e0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 100, 100, None), dtype=tf.float32, name=None), TensorSpec(shape=(None, 100, 100, None), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.float32, name=None))>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ee7651ca-c829-403e-bdc6-6155ec92e293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing partition\n",
    "test_data = data.skip(round(len(data)*.7))\n",
    "test_data = test_data.take(round(len(data)*.3))\n",
    "test_data = test_data.batch(16)\n",
    "test_data = test_data.prefetch(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b974f5f9-9bed-484c-b8ff-5e05578f56f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 100, 100, None), dtype=tf.float32, name=None), TensorSpec(shape=(None, 100, 100, None), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.float32, name=None))>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275b74b0-e801-40db-954d-081e9ca51d7b",
   "metadata": {},
   "source": [
    "# 4. Model Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669bb751-23ed-4c51-a820-251ed2cfeba9",
   "metadata": {},
   "source": [
    "### 4.1 Build Embedding Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc2c972-742d-422e-94e3-c717c8b35719",
   "metadata": {},
   "source": [
    "### 4.2 Build Distance Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad49a5f-2c47-4150-beb0-48e2750ef6cb",
   "metadata": {},
   "source": [
    "### 4.3 Make Siamese Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d447a141-4f91-4b86-86c2-7f4d9456883d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_base_network(input_shape):\n",
    "    input = Input(shape=input_shape)\n",
    "    x = Conv2D(64, (10, 10), activation='relu')(input)\n",
    "    x = MaxPooling2D()(x)\n",
    "    x = Conv2D(128, (7, 7), activation='relu')(x)\n",
    "    x = MaxPooling2D()(x)\n",
    "    x = Conv2D(128, (4, 4), activation='relu')(x)\n",
    "    x = MaxPooling2D()(x)\n",
    "    x = Conv2D(256, (4, 4), activation='relu')(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(4096, activation='sigmoid')(x)\n",
    "    return Model(input, x)\n",
    "\n",
    "def euclidean_distance(vectors):\n",
    "    (featsA, featsB) = vectors\n",
    "    sumSquared = tf.reduce_sum(tf.square(featsA - featsB), axis=1, keepdims=True)\n",
    "    return tf.sqrt(tf.maximum(sumSquared, tf.keras.backend.epsilon()))\n",
    "\n",
    "def build_siamese_network(input_shape):\n",
    "    base_network = create_base_network(input_shape)\n",
    "    \n",
    "    input_a = Input(shape=input_shape)\n",
    "    input_b = Input(shape=input_shape)\n",
    "    \n",
    "    featsA = base_network(input_a)\n",
    "    featsB = base_network(input_b)\n",
    "    \n",
    "    distance = tf.keras.layers.Lambda(euclidean_distance)([featsA, featsB])\n",
    "    \n",
    "    model = Model(inputs=[input_a, input_b], outputs=distance)\n",
    "    return model\n",
    "\n",
    "input_shape = (100, 100, 3)\n",
    "model = build_siamese_network(input_shape)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b245e82-da0c-417b-97cf-7db856f55b25",
   "metadata": {},
   "source": [
    "# 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9dc5e007-df4c-4bfd-95de-79cef591a542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Layer \"functional_3\" expects 2 input(s), but it received 3 input tensors. Inputs received: [<tf.Tensor 'data:0' shape=(None, 100, 100, None) dtype=float32>, <tf.Tensor 'data_1:0' shape=(None, 100, 100, None) dtype=float32>, <tf.Tensor 'data_2:0' shape=(None,) dtype=float32>]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[62], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m train_data_pairs \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mzip((train_data, train_data))  \u001b[38;5;66;03m# Pair each sample with itself for training\u001b[39;00m\n\u001b[0;32m      5\u001b[0m test_data_pairs \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mzip((test_data, test_data))  \u001b[38;5;66;03m# Pair each sample with itself for testing\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data_pairs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_data_pairs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\Face_Recognition\\projectDir\\FaceRecProject\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mE:\\Face_Recognition\\projectDir\\FaceRecProject\\Lib\\site-packages\\keras\\src\\layers\\input_spec.py:160\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    158\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mflatten(inputs)\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(inputs) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(input_spec):\n\u001b[1;32m--> 160\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    161\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLayer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m expects \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(input_spec)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m input(s),\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    162\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but it received \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(inputs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m input tensors. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    163\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInputs received: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minputs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    164\u001b[0m     )\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m input_index, (x, spec) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(inputs, input_spec)):\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: Layer \"functional_3\" expects 2 input(s), but it received 3 input tensors. Inputs received: [<tf.Tensor 'data:0' shape=(None, 100, 100, None) dtype=float32>, <tf.Tensor 'data_1:0' shape=(None, 100, 100, None) dtype=float32>, <tf.Tensor 'data_2:0' shape=(None,) dtype=float32>]"
     ]
    }
   ],
   "source": [
    "# Example data preparation assuming 'train_data' and 'test_data' are paired datasets\n",
    "# Adjust according to your actual data structure\n",
    "\n",
    "train_data_pairs = tf.data.Dataset.zip((train_data, train_data))  # Pair each sample with itself for training\n",
    "test_data_pairs = tf.data.Dataset.zip((test_data, test_data))  # Pair each sample with itself for testing\n",
    "\n",
    "history = model.fit(train_data_pairs, epochs=10, validation_data=test_data_pairs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f40190-91bc-4af7-8e9f-967208e9d6d8",
   "metadata": {},
   "source": [
    "### 5.1 Setup Loss and Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cdc1fc-16b3-427a-ba81-513ebb191aa8",
   "metadata": {},
   "source": [
    "### 5.2 Establish Checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7125dd11-4374-4ee0-ba9a-f31103b05328",
   "metadata": {},
   "source": [
    "### 5.3 Build Train Step Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b90995-af0e-42ce-938b-5181d4fad511",
   "metadata": {},
   "source": [
    "### 5.4 Build Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38505a3-33ad-47a9-ab89-14de91c7c932",
   "metadata": {},
   "source": [
    "### 5.5 Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ee97b8-2a81-414a-b020-42f4ae81c053",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_data, epochs=10, validation_data=test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7338e554-95f2-4144-b554-719cd3096f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "\n",
    "def preprocess_image(image):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    image = cv2.resize(image, (100, 100))\n",
    "    image = image / 255.0\n",
    "    image = np.expand_dims(image, axis=-1)\n",
    "    return np.expand_dims(image, axis=0)\n",
    "\n",
    "# Load known face encodings and names\n",
    "known_encodings = []\n",
    "known_names = []\n",
    "\n",
    "# Encode the images in the POS_PATH as known faces\n",
    "for file_name in os.listdir(POS_PATH):\n",
    "    img_path = os.path.join(POS_PATH, file_name)\n",
    "    img = cv2.imread(img_path)\n",
    "    processed_img = preprocess_image(img)\n",
    "    encoding = model.predict([processed_img, processed_img])\n",
    "    known_encodings.append(encoding)\n",
    "    known_names.append(file_name.split('.')[0])\n",
    "\n",
    "# Real-time face recognition\n",
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened(): \n",
    "    ret, frame = cap.read()\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    faces = face_cascade.detectMultiScale(frame_rgb, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        face = frame_rgb[y:y+h, x:x+w]\n",
    "        processed_face = preprocess_image(face)\n",
    "        encoding = model.predict([processed_face, processed_face])\n",
    "        \n",
    "        min_dist = float('inf')\n",
    "        name = \"Unknown\"\n",
    "        \n",
    "        for i, known_encoding in enumerate(known_encodings):\n",
    "            dist = np.linalg.norm(encoding - known_encoding)\n",
    "            if dist < min_dist:\n",
    "                min_dist = dist\n",
    "                name = known_names[i]\n",
    "        \n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "        cv2.putText(frame, name, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "    \n",
    "    cv2.imshow('Video', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FaceRecProject",
   "language": "python",
   "name": "facerecproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
